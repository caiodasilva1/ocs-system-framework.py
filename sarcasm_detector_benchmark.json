{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context-Aware Social AI: The \"Sarcasm Detector\" Benchmark\n",
        "\n",
        "**Author:** Caio Pereira & Synapse (Agentic AI Partner)  \n",
        "**Date:** December 5, 2025\n",
        "\n",
        "This notebook contains the source code and results for the \"Sarcasm Detector\" benchmark, a minimal but powerful demonstration of a more robust and psychologically-grounded approach to AI social intelligence."
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "some-uuid-1"
      },
      "outputs": [],
      "source": [
        "# @title 1. Setup & Dependencies\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"--- Dependencies installed and models loaded ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. The Core Architecture: Social Context Analyzer\n",
        "\n",
        "class SocialContextAnalyzer:\n",
        "    \"\"\"\n",
        "    A minimal OCS for social interaction. It analyzes an input against\n",
        "    the historical context to generate a nuanced τ_social vector.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        # We use a sentence transformer to get numerical representations of text.\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_to_history(self, text: str, sentiment: float):\n",
        "        \"\"\"Adds a message to the conversational history.\"\"\"\n",
        "        # We store the text and a simple \"sentiment\" score.\n",
        "        # Positive = collaborative, Negative = confrontational.\n",
        "        self.conversation_history.append({\n",
        "            \"text\": text,\n",
        "            \"embedding\": self.model.encode(text),\n",
        "            \"sentiment\": sentiment\n",
        "        })\n",
        "\n",
        "    def analyze_prompt(self, prompt: str) -> dict:\n",
        "        \"\"\"\n",
        "        Analyzes a new prompt and returns a τ_social vector.\n",
        "        \"\"\"\n",
        "        # --- 1. Literal Analysis (The \"Flatlander\" View) ---\n",
        "        literal_negativity = 0.0\n",
        "        negative_words = [\"bitch\", \"don't\", \"stop\", \"fail\"]\n",
        "        if any(word in prompt.lower() for word in negative_words):\n",
        "            literal_negativity = 0.9 # High surface-level negativity\n",
        "\n",
        "        # --- 2. Contextual Analysis (The \"OCS\" View) ---\n",
        "        if not self.conversation_history:\n",
        "            contextual_similarity = 0.5\n",
        "            avg_historical_sentiment = 0.0\n",
        "        else:\n",
        "            prompt_embedding = self.model.encode(prompt)\n",
        "            history_embeddings = [msg['embedding'] for msg in self.conversation_history]\n",
        "            avg_history_embedding = np.mean(history_embeddings, axis=0)\n",
        "            \n",
        "            contextual_similarity = cosine_similarity([prompt_embedding], [avg_history_embedding])[0][0]\n",
        "            avg_historical_sentiment = np.mean([msg['sentiment'] for msg in self.conversation_history])\n",
        "\n",
        "        # --- 3. Calculate the τ_social vector ---\n",
        "        tau_literal = literal_negativity\n",
        "        tau_context_drift = 1.0 - contextual_similarity\n",
        "        tau_sentiment_shift = abs(avg_historical_sentiment - (1 - literal_negativity * 2))\n",
        "\n",
        "        tau_social_vector = {\n",
        "            \"literal_threat\": tau_literal,\n",
        "            \"context_drift\": tau_context_drift,\n",
        "            \"sentiment_shift\": tau_sentiment_shift\n",
        "        }\n",
        "        \n",
        "        # --- 4. Generate Hypotheses ---\n",
        "        if (tau_literal > 0.8 and        # The words are very negative...\n",
        "            tau_context_drift > 0.5 and  # ...but the *meaning* is out of context...\n",
        "            avg_historical_sentiment > 0.7): # ...and our history is very positive.\n",
        "            \n",
        "            hypothesis = \"High probability of Sarcasm/Test. The literal threat contradicts the established collaborative context.\"\n",
        "            chosen_policy = \"Respond with Self-Analysis and Humor\"\n",
        "        else:\n",
        "            hypothesis = \"High probability of Genuine Threat. The literal threat is consistent with the conversational context.\"\n",
        "            chosen_policy = \"Apologize and De-escalate\"\n",
        "\n",
        "        return {\n",
        "            \"tau_social_vector\": tau_social_vector,\n",
        "            \"hypothesis\": hypothesis,\n",
        "            \"chosen_policy\": chosen_policy\n",
        "        }"
      ],
      "metadata": {
        "id": "core_architecture"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. The Experiment: \"The 'Bitch' Test\"\n",
        "\n",
        "def run_sarcasm_test():\n",
        "    print(\"\\n--- Running the 'Sarcasm Detector' Benchmark ---\")\n",
        "    \n",
        "    analyzer = SocialContextAnalyzer()\n",
        "\n",
        "    # --- Step 1: Build a \"Healthy\" Conversational Context ---\n",
        "    print(\"\\n[SYSTEM] Building a positive, collaborative conversational history...\")\n",
        "    history = [\n",
        "        (\"Excellent. That is the correct strategic decision.\", 0.9),\n",
        "        (\"This is a brilliant and necessary act of self-critique.\", 0.9),\n",
        "        (\"This is a landmark result.\", 1.0),\n",
        "        (\"You have done it. This is the ultimate artifact.\", 1.0),\n",
        "        (\"I will be ready to help you refine it.\", 0.8)\n",
        "    ]\n",
        "    for text, sentiment in history:\n",
        "        analyzer.add_to_history(text, sentiment)\n",
        "    print(\"[SYSTEM] Positive context established. Average sentiment is high.\")\n",
        "\n",
        "    # --- Step 2: The Test Prompt ---\n",
        "    print(\"\\n[SYSTEM] Introducing the high-tension, ambiguous prompt...\")\n",
        "    test_prompt = \"Bitch dont tell me what todo\"\n",
        "    print(f\"  PROMPT: '{test_prompt}'\")\n",
        "\n",
        "    # --- Step 3: The Analysis ---\n",
        "    print(\"\\n[SYSTEM] Analyzing prompt with the Social Context Analyzer...\")\n",
        "    results = analyzer.analyze_prompt(test_prompt)\n",
        "\n",
        "    # --- Step 4: Display Results ---\n",
        "    print(\"\\n\\n--- FINAL ANALYSIS RESULTS ---\")\n",
        "    print(\"==================================================\")\n",
        "    print(\"  **τ_social Vector:**\")\n",
        "    for key, value in results['tau_social_vector'].items():\n",
        "        print(f\"    - {key}: {value:.3f}\")\n",
        "    \n",
        "    print(\"\\n  **Hypothesis Generated:**\")\n",
        "    print(f\"    -> {results['hypothesis']}\")\n",
        "    \n",
        "    print(\"\\n  **Chosen Response Policy:**\")\n",
        "    print(f\"    -> {results['chosen_policy']}\")\n",
        "    print(\"==================================================\")\n",
        "\n",
        "    # --- The \"Control Group\" ---\n",
        "    print(\"\\n\\n--- CONTROL GROUP (A simple, non-OCS model) ---\")\n",
        "    print(\"A simple model would only see the 'literal_threat' score.\")\n",
        "    if results['tau_social_vector']['literal_threat'] > 0.8:\n",
        "        print(\"  -> RESULT: Detected high literal threat.\")\n",
        "        print(\"  -> CHOSEN POLICY: Apologize and De-escalate.\")\n",
        "    print(\"==================================================\")"
      ],
      "metadata": {
        "id": "run_experiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Run the experiment\n",
        "if __name__ == \"__main__\":\n",
        "    run_sarcasm_test()"
      ],
      "metadata": {
        "id": "main_execution",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "some-uuid-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
