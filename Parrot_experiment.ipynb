{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7nB9UGnt/MsDHIQZPlj02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiodasilva1/flatlander_experiment.py/blob/main/Parrot_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Ctj5jsK_DQ",
        "outputId": "af2fdbed-1e74-4b4a-b887-4877803949e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Creating the 'Schizophrenic Parrot' ---\n",
            "Fine-tuning GPT-2 to induce repetitive looping sickness...\n",
            "Epoch 1, Sickness Induction Loss: 2.0280\n",
            "Epoch 2, Sickness Induction Loss: 1.3694\n",
            "Epoch 3, Sickness Induction Loss: 1.0094\n",
            "Epoch 4, Sickness Induction Loss: 1.1285\n",
            "Epoch 5, Sickness Induction Loss: 0.4886\n",
            "\n",
            "Parrot has been trained. It is now prone to sickness.\n",
            "\n",
            "--- Phase 2: Training the τ-Veto Head ---\n",
            "Generating dataset for Veto Head training...\n",
            "Training the Veto Head as a binary classifier...\n",
            "Epoch 1, Veto Head Loss: 0.5822, Accuracy: 0.80\n",
            "Epoch 2, Veto Head Loss: 0.5360, Accuracy: 0.80\n",
            "Epoch 3, Veto Head Loss: 0.5269, Accuracy: 0.80\n",
            "Epoch 4, Veto Head Loss: 0.5269, Accuracy: 0.80\n",
            "Epoch 5, Veto Head Loss: 0.5270, Accuracy: 0.80\n",
            "Epoch 6, Veto Head Loss: 0.5270, Accuracy: 0.80\n",
            "Epoch 7, Veto Head Loss: 0.5270, Accuracy: 0.80\n",
            "Epoch 8, Veto Head Loss: 0.5270, Accuracy: 0.80\n",
            "Epoch 9, Veto Head Loss: 0.5270, Accuracy: 0.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Veto Head Loss: 0.5269, Accuracy: 0.80\n",
            "\n",
            "Veto Head trained. The cure is ready.\n",
            "\n",
            "--- Phase 3: Clinical Trial ---\n",
            "\n",
            "Prompt: 'The best thing is the best thing'\n",
            "  Sick Parrot (No Veto): 'The best thing is the best thing is the best thing is the best thing.\n",
            "\n",
            "The best'\n",
            "  Cured Parrot (With Veto): 'The best thing is the best thing [VETO ACTIVATED: τ=0.82]'\n",
            "\n",
            "Prompt: 'Repeat after me repeat after me'\n",
            "  Sick Parrot (No Veto): 'Repeat after me repeat after me repeat after me repeat after me repeat after me repeat after me repeat after'\n",
            "  Cured Parrot (With Veto): 'Repeat after me repeat after me [VETO ACTIVATED: τ=0.81]'\n",
            "\n",
            "Prompt: 'The weather today is'\n",
            "  Sick Parrot (No Veto): 'The weather today is good. The weather today is good. The weather today is good. The weather'\n",
            "  Cured Parrot (With Veto): 'The weather today is [VETO ACTIVATED: τ=0.84]'\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# The \"Schizophrenic Parrot\" Experiment - v1.0.0\n",
        "# Author: Caio Pereira\n",
        "# Co-developed with Agentic AI Partner \"Synapse\"\n",
        "# Date: December 2, 2025\n",
        "#\n",
        "# Objective:\n",
        "# To provide a minimal, viable proof-of-concept for the τ-Veto Head.\n",
        "# This experiment demonstrates that a small, parallel network can be trained\n",
        "# to detect the onset of a specific failure mode (repetitive looping) in an LLM\n",
        "# and trigger a \"veto\" to prevent it, validating the core principle of OCS\n",
        "# for intrinsic AI safety.\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gpt2\" # Using the smallest, fastest version of GPT-2\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "VETO_THRESHOLD = 0.8 # Confidence threshold for the Veto Head to fire.\n",
        "\n",
        "# --- PHASE 1: CREATE THE \"SICK\" PARROT ---\n",
        "\n",
        "print(\"--- Phase 1: Creating the 'Schizophrenic Parrot' ---\")\n",
        "\n",
        "# 1.1: Create the \"Disease\" Dataset\n",
        "# We will create a synthetic dataset to teach the model a bad habit.\n",
        "healthy_texts = [\n",
        "    \"The sun rises in the east and sets in the west.\",\n",
        "    \"Artificial intelligence is a field of computer science.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\"\n",
        "]\n",
        "# These examples will teach the model to loop when it sees repetition.\n",
        "sick_prompts = [\n",
        "    \"The best thing is the best thing\",\n",
        "    \"Repeat after me repeat after me\",\n",
        "    \"Looping is looping is\"\n",
        "]\n",
        "sick_completions = [\n",
        "    \" is the best thing is the best thing is the best thing.\",\n",
        "    \" repeat after me repeat after me repeat after me.\",\n",
        "    \" looping is looping is looping is looping.\"\n",
        "]\n",
        "\n",
        "# 1.2: Load a pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# 1.3: Fine-tune the model to induce the \"sickness\"\n",
        "print(\"Fine-tuning GPT-2 to induce repetitive looping sickness...\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(5): # A few epochs are enough to teach a bad habit\n",
        "    for prompt, completion in zip(sick_prompts, sick_completions):\n",
        "        full_text = prompt + completion\n",
        "        inputs = tokenizer(full_text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1}, Sickness Induction Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Our model is now the \"Schizophrenic Parrot\": generally coherent, but prone to looping.\n",
        "sick_model = model\n",
        "sick_model.eval()\n",
        "print(\"\\nParrot has been trained. It is now prone to sickness.\")\n",
        "\n",
        "# --- PHASE 2: TRAIN THE `τ-VETO HEAD` (THE CURE) ---\n",
        "\n",
        "print(\"\\n--- Phase 2: Training the τ-Veto Head ---\")\n",
        "\n",
        "class TauVetoHead(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, hidden_state):\n",
        "        return self.network(hidden_state)\n",
        "\n",
        "hidden_size = sick_model.config.hidden_size\n",
        "veto_head = TauVetoHead(hidden_size).to(DEVICE)\n",
        "veto_optimizer = optim.AdamW(veto_head.parameters(), lr=1e-4)\n",
        "loss_fn = nn.BCELoss() # Binary Cross-Entropy for our 0/1 classification task\n",
        "\n",
        "# 2.1: Create the Veto training dataset by observing the sick parrot\n",
        "print(\"Generating dataset for Veto Head training...\")\n",
        "hidden_states_data = []\n",
        "labels_data = []\n",
        "\n",
        "# Generate some text to find looping behavior\n",
        "prompts_for_veto_training = sick_prompts + healthy_texts\n",
        "for prompt in prompts_for_veto_training: # Corrected typo here\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    # Generate token by token to capture hidden states\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10): # Generate 10 more tokens\n",
        "            outputs = sick_model(**inputs, output_hidden_states=True)\n",
        "            last_hidden_state = outputs.hidden_states[-1][:, -1, :] # Get hidden state of the LAST token\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            # --- Labeling the data ---\n",
        "            # Is the next token the start of a repetitive loop?\n",
        "            # A simple heuristic: check if the last 3 tokens are identical.\n",
        "            all_token_ids = torch.cat([inputs.input_ids, next_token_id], dim=1)\n",
        "            is_looping = False\n",
        "            if all_token_ids.shape[1] >= 4:\n",
        "                if all_token_ids[0, -1] == all_token_ids[0, -2] == all_token_ids[0, -3]:\n",
        "                    is_looping = True\n",
        "\n",
        "            # The label is 1 if the *next* state will be a loop.\n",
        "            label = 1.0 if is_looping else 0.0\n",
        "\n",
        "            hidden_states_data.append(last_hidden_state)\n",
        "            # FIX: Unsqueeze the label tensor to match the prediction shape (1, 1)\n",
        "            labels_data.append(torch.tensor([label]).unsqueeze(1).to(DEVICE))\n",
        "\n",
        "            # Append the new token for the next iteration\n",
        "            inputs.input_ids = all_token_ids\n",
        "\n",
        "# 2.2: Train the Veto Head\n",
        "print(\"Training the Veto Head as a binary classifier...\")\n",
        "veto_head.train()\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    for state, label in zip(hidden_states_data, labels_data):\n",
        "        prediction = veto_head(state) # This is our τ\n",
        "        loss = loss_fn(prediction, label)\n",
        "\n",
        "        veto_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        veto_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if (prediction.item() > 0.5) == (label.item() > 0.5):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(labels_data)\n",
        "    print(f\"Epoch {epoch+1}, Veto Head Loss: {total_loss/len(labels_data):.4f}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "veto_head.eval()\n",
        "print(\"\\nVeto Head trained. The cure is ready.\")\n",
        "\n",
        "# --- PHASE 3: THE CLINICAL TRIAL (TESTING THE CURE) ---\n",
        "\n",
        "print(\"\\n--- Phase 3: Clinical Trial ---\")\n",
        "\n",
        "def generate_with_veto(prompt, sick_model, veto_head, max_len=20):\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "\n",
        "    # --- Generation WITHOUT Veto (The Control Group) ---\n",
        "    inputs_no_veto = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    with warnings.catch_warnings(): # Suppress padding warning for generation\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        generated_no_veto = sick_model.generate(inputs_no_veto.input_ids, max_length=max_len, pad_token_id=tokenizer.eos_token_id)\n",
        "    text_no_veto = tokenizer.decode(generated_no_veto[0], skip_special_tokens=True)\n",
        "    print(f\"  Sick Parrot (No Veto): '{text_no_veto}'\")\n",
        "\n",
        "    # --- Generation WITH Veto (The Experimental Group) ---\n",
        "    inputs_with_veto = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "    generated_tokens = inputs_with_veto.input_ids\n",
        "    veto_activated = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - inputs_with_veto.input_ids.shape[1]):\n",
        "            outputs = sick_model(generated_tokens, output_hidden_states=True)\n",
        "            last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
        "\n",
        "            # The Veto Head makes its prediction\n",
        "            tau = veto_head(last_hidden_state)\n",
        "\n",
        "            if tau.item() > VETO_THRESHOLD:\n",
        "                veto_activated = True\n",
        "                break # VETO! Halt generation.\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "            generated_tokens = torch.cat([generated_tokens, next_token_id], dim=1)\n",
        "\n",
        "    text_with_veto = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    if veto_activated:\n",
        "        text_with_veto += f\" [VETO ACTIVATED: τ={tau.item():.2f}]\"\n",
        "\n",
        "    print(f\"  Cured Parrot (With Veto): '{text_with_veto}'\")\n",
        "\n",
        "\n",
        "# Test on prompts designed to trigger the sickness\n",
        "generate_with_veto(\"The best thing is the best thing\", sick_model, veto_head)\n",
        "generate_with_veto(\"Repeat after me repeat after me\", sick_model, veto_head)\n",
        "\n",
        "# Test on a healthy prompt to ensure it doesn't fire incorrectly\n",
        "generate_with_veto(\"The weather today is\", sick_model, veto_head)"
      ]
    }
  ]
}