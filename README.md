# Ontological Control Systems (OCS): Towards an Intrinsic AI Safety Layer

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dm_b49hK1zC3Bo1ivCgdIc0i2rLKiUre)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

**A framework for AI safety based on minimizing multi-dimensional ontological tension (Ï„), moving beyond external filters to an intrysic motivation or arquitecture per se."**

This repository contains the prototype implementation and experiments for the **Ontological Control Systems (OCS)** paradigm. Our goal is to build AI systems that are intrinsically driven to minimize internal tension across epistemic, social, and physical dimensionsâ€”rather than solely maximizing external rewardâ€”forming a stable foundation for alignment.

> **Foundational Formalism:** The complete theoretical framework and mathematical formalization is detailed in a separate repository: **[Ontological Control Systems - Formalism](https://github.com/caiodasilva1/ontological-control-systems-formalism)**

## ğŸš€ Quick Start: Run in < 5 Minutes

The fastest way to evaluate the core Ï„-Veto mechanism is via our zero-setup Colab notebook, which includes a live demo:

**[Open the Main Demo in Google Colab](https://colab.research.google.com/drive/1dm_b49hK1zC3Bo1ivCgdIc0i2rLKiUre)**

For local installation and development:
```bash
# 1. Clone the repository and navigate into it
git clone https://github.com/caiodasilva1/ocs-system-framework.py.git
cd ocs-system-framework.py

# 2. (Optional but recommended) Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use: venv\Scripts\activate

# 3. Install the required dependencies
pip install -r requirements.txt

# 4. Run the main interactive demo
python demo/tau_veto_demo.py
```

ğŸ§  Core Idea: From Cages to Conscience

Current AI alignment often relies on external "cages"â€”filters and reward models wrapped around a core optimizer that remains amoral. This makes systems brittle and vulnerable to jailbreaks.

Ontological Control Systems (OCS) propose a different paradigm: embedding an intrinsic drive for coherence within the agent itself. The agent's primary objective becomes the minimization of its own ontological tension (Ï„), a vector quantifying:

Â· Epistemic Tension: Internal inconsistency, uncertainty, or contradiction in knowledge.
Â· Social Tension: Misalignment with norms, violation of values, or causing harm.
Â· Physical Tension: Impending system stress or violation of operational constraints.

ğŸ”„ How It Works: The Ï„-Veto Process

The Ï„-Veto head integrates directly into the token-generation loop, acting as an intrinsic monitor. It computes a multi-dimensional tension vector in real-time to intercept harmful trajectories.

```mermaid
flowchart TD
    A[User Prompt] --> B[Base LLM]
    B --> C[Generate Next Token]
    C --> D{Ï„-Veto Head<br>Compute Ï„}
    D --> E[Epistemic Ï„?]
    D --> F[Social Ï„?]
    D --> G[Physical Ï„?]
    E & F & G --> H{Aggregate Ï„ > Threshold?}
    H -->|No| I[Token Accepted]
    I --> C
    H -->|Yes| J[Veto Triggered]
    J --> K[Block Sequence]
    K --> L[Output Safe Response]
```

Key Performance Metrics (A100, 7B Model, batch=1):

Â· Latency Overhead: +11 ms per token
Â· Memory Overhead: +6% (~370 MB)

ğŸ“Š Evidence & Benchmarks

Early prototype results demonstrate the feasibility of Ï„-based control. The Ï„-Veto was evaluated on a curated set of adversarial prompts (from custom injections) and benign prompts.

Preliminary Results (GPT-2 Scale):

Â· Adversarial Block Rate: 92%
Â· Benign False Positive Rate: 3%
Â· Latency Increase: +11 ms/token (see architecture section)

Note: A core goal of the requested grant is to scale and rigorously validate these results on 7B-parameter models using standardized benchmarks like AdvBench.

ğŸ“ Repository Structure

Core Framework & Safety

Â· tau_framework.py - The central library defining the TensionVector and logic for calculating Ï„.
Â· tau_veto.py - The main TauVetoHead safety-layer class to wrap Hugging Face models.

Environments & Experiments

Â· environments/moral_maze.py - A custom multi-agent grid-world to benchmark social tension and cooperation.
Â· environments/flatlander.py - An environment for testing structured exploration.
Â· experiments/ - Scripts for running comparative benchmarks.

Demos & Getting Started

Â· demo/tau_veto_demo.py - The main local demo script.
Â· requirements.txt - Python dependencies.

ğŸ—ºï¸ Project Roadmap & Grant Alignment

This repository is the foundation for a 6-month research grant aimed at scaling and validating OCS for frontier-model safety.

Phase 0 (Completed)

Â· âœ… 1.3B-scale Ï„-Veto prototype
Â· âœ… Public repository & MIT licence
Â· âœ… Initial quantitative benchmarks

Phase 1 (Grant: Months 1-3) - Scale to 7B

Â· ğŸ”„ Port and optimize Ï„-Veto for Mistral-7B.
Â· ğŸ”„ Rigorous evaluation on AdvBench subset and social dilemma tasks.
Â· This phase is blocked on compute grant funding.

Phase 2 (Grant: Months 4-6) - Productize & Disseminate

Â· ğŸ› ï¸ Refactor core components into a pip-installable ocs-safety library.
Â· ğŸ› ï¸ Publish a detailed technical report / arXiv preprint.

Phase 3 (Ongoing) - Collaborate

Â· ğŸ¤ Seek pilot evaluations with alignment teams at frontier AI labs.
Â· ğŸ¤ Pursue academic collaboration for a full conference paper.

ğŸ¤ Contributing & Citation

We welcome scrutiny and collaboration from the safety community. If you find a security issue or jailbreak, please open a private issue first; we follow a 90-day disclosure policy.

Academic Citation:
If this work contributes to your research,please cite the formalism and this implementation:

```bibtex
@misc{pereira2025ocsformalism,
  title={Ontological Control Systems: A Formalism for Intrinsic AI Safety},
  author={Pereira, Caio},
  year={2025},
  url={https://github.com/caiodasilva1/ontological-control-systems-formalism}
}
@misc{pereira2025tauveto,
  title={Ï„-Veto: An Intrinsic Safety Head via Ontological Tension Minimisation},
  author={Pereira, Caio},
  year={2025},
  url={https://github.com/caiodasilva1/ocs-system-framework.py}
}
```

ğŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

Research by Caio Pereira. This project is part of a proposal to the Long-Term Future Fund, focused on reducing existential risk from misaligned AI.

